{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da654e6f-71fa-47f5-bb17-811db2692dae",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4198d4-37f0-47a3-bd99-d329da25510c",
   "metadata": {},
   "source": [
    "## Audio data\n",
    "    - wav data와 이와 매칭되는 json label data를 model input이 가능하게 변환 필요\n",
    "    - WhisperFeatureExtractor는 16kHz로 sampling된 데이터를 입력받는다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67809c35-5bb3-4c29-aa44-7fef1f564cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set GPU\n",
    "# torch 보다 먼저 import 해야 아래 사항이 적용된다\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"  # Set the GPU 1 to use\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ae4329-9568-471c-ad32-5030b52096d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datasets import load_dataset\n",
    "\n",
    "# File path\n",
    "training_file_dir = \"/data/freetalk_senior/1.Training/\"\n",
    "\n",
    "training_raw_data = []\n",
    "training_raw_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"raw_data/1.AI챗봇/1.AI챗봇_2_자유대화(노인남여)_TRAINING/노인남여_노인대화12_M_1534748575_63_강원_실내/*.wav\"), recursive=True)))\n",
    "training_raw_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"raw_data/1.AI챗봇/1.AI챗봇_2_자유대화(노인남여)_TRAINING/노인남여_노인대화12_M_1539661748_64_수도권_실내/*.wav\"), recursive=True)))\n",
    "training_raw_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"raw_data/1.AI챗봇/1.AI챗봇_2_자유대화(노인남여)_TRAINING/노인남여_노인대화12_M_1539661748_64_수도권_실외/*.wav\"), recursive=True)))\n",
    "\n",
    "training_labeled_data = []\n",
    "training_labeled_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"labeled_data/1.AI챗봇/1.AI챗봇_라벨링_자유대화(노인남여)_TRAINING/노인남여_노인대화12_M_1534748575_63_강원_실내/*.json\"), recursive=True)))\n",
    "training_labeled_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"labeled_data/1.AI챗봇/1.AI챗봇_라벨링_자유대화(노인남여)_TRAINING/노인남여_노인대화12_M_1539661748_64_수도권_실내/*.json\"), recursive=True)))\n",
    "training_labeled_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"labeled_data/1.AI챗봇/1.AI챗봇_라벨링_자유대화(노인남여)_TRAINING/노인남여_노인대화12_M_1539661748_64_수도권_실외/*.json\"), recursive=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e22941-41f6-4743-8958-b673aea0fd50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2086"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_raw_data[0] + training_raw_data[1] + training_raw_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e81e8ed-7e0f-4345-a64a-a2742b7126d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2086"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_labeled_data[0]+ training_labeled_data[1] + training_labeled_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e834e03-c209-45e0-a319-aaa9f69e00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train_input\": training_raw_data,    \n",
    "    \"train_label\": training_labeled_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6cdc9fc-e27b-4c6a-a25b-d796bb6326b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f842e1b87cc447b1ae08df82c1220a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad78d0754adb4d7faddae1d714477efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeee5be3daa94233b24a18c499a2218e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio'],\n",
       "        num_rows: 2086\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load Dataset\n",
    "from datasets import Features, Audio\n",
    "import datasets \n",
    "\n",
    "# load_dataset 이 새로 만들어 버리는 label column 때문에 에러 발생하여 features명시\n",
    "features = Features({\n",
    "    'audio': Audio()\n",
    "})\n",
    "\n",
    "\n",
    "train_input_0 = load_dataset(\"audiofolder\", \n",
    "                           data_files=data_files['train_input'][0], \n",
    "                           features=features)\n",
    "train_input_1 = load_dataset(\"audiofolder\", \n",
    "                           data_files=data_files['train_input'][1], \n",
    "                           features=features)\n",
    "train_input_2 = load_dataset(\"audiofolder\", \n",
    "                           data_files=data_files['train_input'][2], \n",
    "                           features=features)\n",
    "\n",
    "train_input = datasets.DatasetDict({\n",
    "    \"train\": datasets.concatenate_datasets([train_input_0['train'], \n",
    "                                            train_input_1['train'], \n",
    "                                            train_input_2['train']], axis=0)\n",
    "})\n",
    "\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454fbe0f-79ae-4f0c-8bf3-99c75c6450a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603f9c4b2f964729b92ff3ec7768b971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c824529c8b224f7d8ee7a207b9f01291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59a27cd179e4b8ba7cda1903b0aca15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['발화정보', '대화정보', '녹음자정보'],\n",
       "        num_rows: 2086\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets \n",
    "\n",
    "train_label_0 = load_dataset(\"json\", data_files=data_files[\"train_label\"][0])\n",
    "train_label_1 = load_dataset(\"json\", data_files=data_files[\"train_label\"][1])\n",
    "train_label_2 = load_dataset(\"json\", data_files=data_files[\"train_label\"][2])\n",
    "\n",
    "train_label = datasets.DatasetDict({\n",
    "    \"train\": datasets.concatenate_datasets([train_label_0['train'], \n",
    "                                            train_label_1['train'], \n",
    "                                            train_label_2['train']], axis=0)\n",
    "})\n",
    "\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2766b83c-d8fa-45ea-9b12-e1bb77d3e5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', '발화정보', '대화정보', '녹음자정보'],\n",
       "        num_rows: 2086\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.DatasetDict({\n",
    "    \"train\": datasets.concatenate_datasets([train_input['train'], \n",
    "                                            train_label['train']], axis=1)\n",
    "})\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d78cf69-2ab7-4202-a3ae-48303c3352e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_dataset = train_dataset.shuffle(seed=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7215a6d6-a3d7-4d62-b9fb-9cc8063a5cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대화정보', '녹음자정보']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senior_dataset.column_names[\"train\"][2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a7ef1-1bff-484a-b7e9-889a9b99e9e9",
   "metadata": {},
   "source": [
    "# 2. Set Toknizer & FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9a143-1b65-4075-9063-b07012c1be8f",
   "metadata": {},
   "source": [
    "## Feature Extractor: Audio raw data에 대 preprocessing 한다\n",
    "    - Transformers.WhisperFeatureExtractor\n",
    "    1. Audio sample에 대해 padding 및 truncating을 해 모든 sample길이를 30초 맞춘다.\n",
    "        whisper model은 attention mask없이 발화 신호로부터 무시해야 할 input을 직접 추론하도록 학습되었다\n",
    "    2. Audio array를 log-Mel spectrum으로 변환. -> 이 것이 model input\n",
    "\n",
    "## Model: seq-to-seq mapping\n",
    "\n",
    "## Tokeninzer: model output을 text format으로 postprocessing\n",
    "    - Transformers.WhisperTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a871b2-2b0b-468a-91da-1b2b8349a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"jiwon65/whisper-small_korean-zeroth\"\n",
    "\n",
    "# Whisper model output은 vocabulary 단어의 index\n",
    "# 이를 실제 문자와 mapping하기 위해 Tokenizer 사용\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(checkpoint, language=\"Korean\", task=\"transcribe\")\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint, language=\"Korean\", task=\"transcribe\")\n",
    "\n",
    "# Feature Extractor\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "# jiwon65/whisper-small_korean-zeroth model을 fine tuning\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(checkpoint)\n",
    "# feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0369551c-98ca-4aa9-a86c-9f1bf2b761e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# WhisperProcessor: 위 두 모듈을 하나로 묶는다\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint, language=\"Korean\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e197a71-7eb4-485c-9d2b-e303ed0406ea",
   "metadata": {},
   "source": [
    "# 3. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d10886-aad3-4dae-bf25-29a32b015353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(batch):\n",
    "    audio=batch[\"audio\"]\n",
    "    label=batch[\"발화정보\"]\n",
    "    \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    batch[\"labels\"] = tokenizer(label['stt']).input_ids\n",
    "    return batch\n",
    "\n",
    "senior_dataset = senior_dataset.map(preprocess_dataset, \n",
    "                                    remove_columns=senior_dataset.column_names[\"train\"], \n",
    "                                    # batched=True,\n",
    "                                    # batch_size=1,\n",
    "                                    # num_proc=os.cpu_count()\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5208868-14e9-4011-af5d-1ea733782ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_dataset = senior_dataset.shuffle(seed=44)\n",
    "splited_senior_dataset = senior_dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e17169f-51ba-41c6-b0a6-e5e35e91068b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 1668\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 418\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_senior_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9985b-bd7d-4f08-990c-59a040f12f09",
   "metadata": {},
   "source": [
    "# 4. Set Trainer & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57264ea9-63ba-4f3d-9bfe-9a492fa9d9fc",
   "metadata": {},
   "source": [
    "## 4-1. Set DataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d6e60-8d7d-4ffe-9912-fc836926cd1d",
   "metadata": {},
   "source": [
    "    seq-to-seq model을 위한 DataCollator는 input_feature와 label을 독립적으로 다룬다는 점에서 특징적.\n",
    "        input_feature -> FeatureExtractor\n",
    "        label -> Tokenizer 로 다뤄야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65a0d4c2-c3ba-4510-8421-9aedf7af51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_feature -> log_mel_spectrogram -> PyTorch Tensor\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# DatasetDict -> torch.tensor\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 인풋 데이터와 라벨 데이터의 길이가 다르며, 따라서 서로 다른 패딩 방법이 적용되어야 한다. 그러므로 두 데이터를 분리해야 한다.\n",
    "        # 먼저 오디오 인풋 데이터를 간단히 토치 텐서로 반환하는 작업을 수행한다.\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # Tokenize된 레이블 시퀀스를 가져온다.\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # 레이블 시퀀스에 대해 최대 길이만큼 패딩 작업을 실시한다.\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # 패딩 토큰을 -100으로 치환하여 loss 계산 과정에서 무시되도록 한다.\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        # 이전 토크나이즈 과정에서 bos 토큰이 추가되었다면 bos 토큰을 잘라낸다.\n",
    "        # 해당 토큰은 이후 언제든 추가할 수 있다.\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41c592c9-e72e-4aad-9697-8181b4cbba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint, language=\"Korean\", task=\"transcribe\")\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be7cce-07f7-4832-aea8-6589b2265897",
   "metadata": {},
   "source": [
    "## 4-2. Set metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f22e791d-29a1-4d75-90c7-1b61c027a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('cer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29721f54-2358-4b48-b400-635117262c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    label_ids[label_ids == -11] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa980e-0cd0-4e6f-a413-0745fad7dab4",
   "metadata": {},
   "source": [
    "## 4-3. Set Train args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2b4a2-9963-4bbe-a79e-a369907784b8",
   "metadata": {},
   "source": [
    "    output_dir: model weight를 저장하는 경로\n",
    "    generation_max_length: evaluation동안 recursive하게 생성되는 token들의 최대 길이 \n",
    "    save_steps: Training동안, 이 parameter에 설정한 step 마다 checkpoint 저장\n",
    "    eval_steps: Training동안, 이 parameter에 설정한 step 마다 checkpoint 평가 수행\n",
    "    report_to: training_log를 어디 저장할 지 설정. default: tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3af3d-e381-4693-aa6e-9946f7fed1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model load\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35a868cf-3929-4fa9-8199-03188f02ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdcc62de-ea8b-44ad-8564-935abac775a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wooseok/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "steps = 500\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./test_trainer\",       # output dir\n",
    "    # logging_dir=\"./logging_dir\",       # storing to log_dir\n",
    "    max_steps=steps,                   # epoch 대신 설정\n",
    "    # num_train_epochs = 5,              # total num of training epochs\n",
    "    evaluation_strategy=\"steps\",       # evaluate after each \"steps\"\n",
    "\n",
    "    warmup_steps=steps/20,             # num of warmup steps for learning rate scheduler\n",
    "    save_steps=steps/10,\n",
    "    eval_steps=steps/20,\n",
    "    logging_steps=steps/50,            # how often to print log\n",
    "    \n",
    "    per_device_train_batch_size=16,    # batch size of per_device during \n",
    "    per_device_eval_batch_size=16,      # batch size per GPU for evaluation\n",
    "    # per_device_eval_batch_size=10,   # batch size for evaluation\n",
    "    gradient_accumulation_steps=2,     # tot num of steps before back propagation. train time increase.\n",
    "    learning_rate=1e-5,                # initial lr for AdamW optimizer\n",
    "    # weight_decay=0.01,                 # strength of weight decay\n",
    "    gradient_checkpointing=True,       # saving mem, but make backward pass slower\n",
    "    fp16=True,                         # use mixed precision\n",
    "    # fp16_opt_level=\"02\",               # mixed precision mode\n",
    "    \n",
    "    # predict_with_generate=True,        # using generate to calculate 'generative metrics'(ROUGE, BLEU)\n",
    "    # generation_max_length=225,         # generative metrics's max len\n",
    "\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,       # best checkpoint will always be saved\n",
    "    \n",
    "    metric_for_best_model=\"cer\",       # 한국어의 경우 띄어쓰기가 애매한 경우가 많아서 'wer'보다는 'cer'이 더 적합할 것\n",
    "    greater_is_better=False,           # is better metric result is big?\n",
    "    push_to_hub=False,\n",
    "    # do_train=True,                   # Perform training\n",
    "    # do_eval=True,                    # Perform evaluation\n",
    "    run_name=\"test1800Training\",       # experiment name\n",
    "    # seed=3,                            # seed for experiment reproductbility 3x3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282679dc-f654-4767-bd2d-bf025b457117",
   "metadata": {},
   "source": [
    "## 4-4. Set Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccd798f9-c061-4c3e-8042-0c1bfd98126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=splited_senior_dataset[\"train\"],\n",
    "    eval_dataset=splited_senior_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fca102-0471-47a7-a0ae-a9d234213870",
   "metadata": {},
   "source": [
    "## 4-5. Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ec090ca-4861-4c87-b63c-66a4f0a3d344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 맨 처음 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b787f2b1-e385-43c8-9c27-22dd5cc3b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
      "CUDA_VISIBLE_DEVICES: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA_DEVICE_ORDER:\", os.environ.get(\"CUDA_DEVICE_ORDER\"))\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57091c-edcf-488c-aef8-a4452d526bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch 를 먼저 import 하면 CUDA_VISIBLE_DEVICES 설정이 안먹음\n",
    "import torch\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.config.use_cache = False # mem usage 줄이기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c477a3d9-6844-4c4e-8f5d-b661427a8bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aad58b-b915-428f-8106-928e7e4dd674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train() # data 1800 개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4339e0b5-07b5-48f9-9f89-9524decb167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = WhisperForConditionalGeneration.from_pretrained(\"./test_trainer/checkpoint-150/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9829c2a-3878-45cf-8246-54ea6a447cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model_tuned,\n",
    "    train_dataset=splited_senior_dataset[\"train\"],\n",
    "    eval_dataset=splited_senior_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f497515-7f71-4239-8775-c51a294f81e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 01:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15303942561149597,\n",
       " 'eval_cer': 7.777777777777778,\n",
       " 'eval_runtime': 106.0101,\n",
       " 'eval_samples_per_second': 3.943,\n",
       " 'eval_steps_per_second': 0.5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4210fdab-69d6-4ca9-9cb8-210b624b2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw = WhisperForConditionalGeneration.from_pretrained(\"jiwon65/whisper-small_korean-zeroth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "794d049f-3975-4ab2-9397-d0625823a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = model_raw.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5450de5b-3143-4f53-9828-3c54b72e0f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1474889516830444, 'eval_cer': 29.860017497812773}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_kor",
   "language": "python",
   "name": "whisper_kor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
