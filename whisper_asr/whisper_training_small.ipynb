{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da654e6f-71fa-47f5-bb17-811db2692dae",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4198d4-37f0-47a3-bd99-d329da25510c",
   "metadata": {},
   "source": [
    "## Audio data\n",
    "    - wav dataì™€ ì´ì™€ ë§¤ì¹­ë˜ëŠ” json label dataë¥¼ model inputì´ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜ í•„ìš”\n",
    "    - WhisperFeatureExtractorëŠ” 16kHzë¡œ samplingëœ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67809c35-5bb3-4c29-aa44-7fef1f564cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set GPU\n",
    "# torch ë³´ë‹¤ ë¨¼ì € import í•´ì•¼ ì•„ë˜ ì‚¬í•­ì´ ì ìš©ëœë‹¤\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"  # Set the GPU 1 to use\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ae4329-9568-471c-ad32-5030b52096d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datasets import load_dataset\n",
    "\n",
    "# File path\n",
    "training_file_dir = \"/data/freetalk_senior/1.Training/\"\n",
    "\n",
    "training_raw_data = []\n",
    "training_raw_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"raw_data/1.AIì±—ë´‡/1.AIì±—ë´‡_2_ììœ ëŒ€í™”(ë…¸ì¸ë‚¨ì—¬)_TRAINING/ë…¸ì¸ë‚¨ì—¬_ë…¸ì¸ëŒ€í™”12_M_1534748575_63_ê°•ì›_ì‹¤ë‚´/*.wav\"), recursive=True)))\n",
    "training_raw_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"raw_data/1.AIì±—ë´‡/1.AIì±—ë´‡_2_ììœ ëŒ€í™”(ë…¸ì¸ë‚¨ì—¬)_TRAINING/ë…¸ì¸ë‚¨ì—¬_ë…¸ì¸ëŒ€í™”12_M_1539661748_64_ìˆ˜ë„ê¶Œ_ì‹¤ë‚´/*.wav\"), recursive=True)))\n",
    "training_raw_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"raw_data/1.AIì±—ë´‡/1.AIì±—ë´‡_2_ììœ ëŒ€í™”(ë…¸ì¸ë‚¨ì—¬)_TRAINING/ë…¸ì¸ë‚¨ì—¬_ë…¸ì¸ëŒ€í™”12_M_1539661748_64_ìˆ˜ë„ê¶Œ_ì‹¤ì™¸/*.wav\"), recursive=True)))\n",
    "\n",
    "training_labeled_data = []\n",
    "training_labeled_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"labeled_data/1.AIì±—ë´‡/1.AIì±—ë´‡_ë¼ë²¨ë§_ììœ ëŒ€í™”(ë…¸ì¸ë‚¨ì—¬)_TRAINING/ë…¸ì¸ë‚¨ì—¬_ë…¸ì¸ëŒ€í™”12_M_1534748575_63_ê°•ì›_ì‹¤ë‚´/*.json\"), recursive=True)))\n",
    "training_labeled_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"labeled_data/1.AIì±—ë´‡/1.AIì±—ë´‡_ë¼ë²¨ë§_ììœ ëŒ€í™”(ë…¸ì¸ë‚¨ì—¬)_TRAINING/ë…¸ì¸ë‚¨ì—¬_ë…¸ì¸ëŒ€í™”12_M_1539661748_64_ìˆ˜ë„ê¶Œ_ì‹¤ë‚´/*.json\"), recursive=True)))\n",
    "training_labeled_data.append(sorted(glob.glob(os.path.join(training_file_dir, \"labeled_data/1.AIì±—ë´‡/1.AIì±—ë´‡_ë¼ë²¨ë§_ììœ ëŒ€í™”(ë…¸ì¸ë‚¨ì—¬)_TRAINING/ë…¸ì¸ë‚¨ì—¬_ë…¸ì¸ëŒ€í™”12_M_1539661748_64_ìˆ˜ë„ê¶Œ_ì‹¤ì™¸/*.json\"), recursive=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e22941-41f6-4743-8958-b673aea0fd50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2086"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_raw_data[0] + training_raw_data[1] + training_raw_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e81e8ed-7e0f-4345-a64a-a2742b7126d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2086"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_labeled_data[0]+ training_labeled_data[1] + training_labeled_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e834e03-c209-45e0-a319-aaa9f69e00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train_input\": training_raw_data,    \n",
    "    \"train_label\": training_labeled_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6cdc9fc-e27b-4c6a-a25b-d796bb6326b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f842e1b87cc447b1ae08df82c1220a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad78d0754adb4d7faddae1d714477efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeee5be3daa94233b24a18c499a2218e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio'],\n",
       "        num_rows: 2086\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load Dataset\n",
    "from datasets import Features, Audio\n",
    "import datasets \n",
    "\n",
    "# load_dataset ì´ ìƒˆë¡œ ë§Œë“¤ì–´ ë²„ë¦¬ëŠ” label column ë•Œë¬¸ì— ì—ëŸ¬ ë°œìƒí•˜ì—¬ featuresëª…ì‹œ\n",
    "features = Features({\n",
    "    'audio': Audio()\n",
    "})\n",
    "\n",
    "\n",
    "train_input_0 = load_dataset(\"audiofolder\", \n",
    "                           data_files=data_files['train_input'][0], \n",
    "                           features=features)\n",
    "train_input_1 = load_dataset(\"audiofolder\", \n",
    "                           data_files=data_files['train_input'][1], \n",
    "                           features=features)\n",
    "train_input_2 = load_dataset(\"audiofolder\", \n",
    "                           data_files=data_files['train_input'][2], \n",
    "                           features=features)\n",
    "\n",
    "train_input = datasets.DatasetDict({\n",
    "    \"train\": datasets.concatenate_datasets([train_input_0['train'], \n",
    "                                            train_input_1['train'], \n",
    "                                            train_input_2['train']], axis=0)\n",
    "})\n",
    "\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454fbe0f-79ae-4f0c-8bf3-99c75c6450a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603f9c4b2f964729b92ff3ec7768b971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c824529c8b224f7d8ee7a207b9f01291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59a27cd179e4b8ba7cda1903b0aca15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ë°œí™”ì •ë³´', 'ëŒ€í™”ì •ë³´', 'ë…¹ìŒìì •ë³´'],\n",
       "        num_rows: 2086\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets \n",
    "\n",
    "train_label_0 = load_dataset(\"json\", data_files=data_files[\"train_label\"][0])\n",
    "train_label_1 = load_dataset(\"json\", data_files=data_files[\"train_label\"][1])\n",
    "train_label_2 = load_dataset(\"json\", data_files=data_files[\"train_label\"][2])\n",
    "\n",
    "train_label = datasets.DatasetDict({\n",
    "    \"train\": datasets.concatenate_datasets([train_label_0['train'], \n",
    "                                            train_label_1['train'], \n",
    "                                            train_label_2['train']], axis=0)\n",
    "})\n",
    "\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2766b83c-d8fa-45ea-9b12-e1bb77d3e5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'ë°œí™”ì •ë³´', 'ëŒ€í™”ì •ë³´', 'ë…¹ìŒìì •ë³´'],\n",
       "        num_rows: 2086\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.DatasetDict({\n",
    "    \"train\": datasets.concatenate_datasets([train_input['train'], \n",
    "                                            train_label['train']], axis=1)\n",
    "})\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d78cf69-2ab7-4202-a3ae-48303c3352e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_dataset = train_dataset.shuffle(seed=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7215a6d6-a3d7-4d62-b9fb-9cc8063a5cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ëŒ€í™”ì •ë³´', 'ë…¹ìŒìì •ë³´']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senior_dataset.column_names[\"train\"][2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a7ef1-1bff-484a-b7e9-889a9b99e9e9",
   "metadata": {},
   "source": [
    "# 2. Set Toknizer & FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9a143-1b65-4075-9063-b07012c1be8f",
   "metadata": {},
   "source": [
    "## Feature Extractor: Audio raw dataì— ëŒ€ preprocessing í•œë‹¤\n",
    "    - Transformers.WhisperFeatureExtractor\n",
    "    1. Audio sampleì— ëŒ€í•´ padding ë° truncatingì„ í•´ ëª¨ë“  sampleê¸¸ì´ë¥¼ 30ì´ˆ ë§ì¶˜ë‹¤.\n",
    "        whisper modelì€ attention maskì—†ì´ ë°œí™” ì‹ í˜¸ë¡œë¶€í„° ë¬´ì‹œí•´ì•¼ í•  inputì„ ì§ì ‘ ì¶”ë¡ í•˜ë„ë¡ í•™ìŠµë˜ì—ˆë‹¤\n",
    "    2. Audio arrayë¥¼ log-Mel spectrumìœ¼ë¡œ ë³€í™˜. -> ì´ ê²ƒì´ model input\n",
    "\n",
    "## Model: seq-to-seq mapping\n",
    "\n",
    "## Tokeninzer: model outputì„ text formatìœ¼ë¡œ postprocessing\n",
    "    - Transformers.WhisperTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a871b2-2b0b-468a-91da-1b2b8349a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"jiwon65/whisper-small_korean-zeroth\"\n",
    "\n",
    "# Whisper model outputì€ vocabulary ë‹¨ì–´ì˜ index\n",
    "# ì´ë¥¼ ì‹¤ì œ ë¬¸ìì™€ mappingí•˜ê¸° ìœ„í•´ Tokenizer ì‚¬ìš©\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(checkpoint, language=\"Korean\", task=\"transcribe\")\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint, language=\"Korean\", task=\"transcribe\")\n",
    "\n",
    "# Feature Extractor\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "# jiwon65/whisper-small_korean-zeroth modelì„ fine tuning\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(checkpoint)\n",
    "# feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0369551c-98ca-4aa9-a86c-9f1bf2b761e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# WhisperProcessor: ìœ„ ë‘ ëª¨ë“ˆì„ í•˜ë‚˜ë¡œ ë¬¶ëŠ”ë‹¤\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint, language=\"Korean\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e197a71-7eb4-485c-9d2b-e303ed0406ea",
   "metadata": {},
   "source": [
    "# 3. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d10886-aad3-4dae-bf25-29a32b015353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(batch):\n",
    "    audio=batch[\"audio\"]\n",
    "    label=batch[\"ë°œí™”ì •ë³´\"]\n",
    "    \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    batch[\"labels\"] = tokenizer(label['stt']).input_ids\n",
    "    return batch\n",
    "\n",
    "senior_dataset = senior_dataset.map(preprocess_dataset, \n",
    "                                    remove_columns=senior_dataset.column_names[\"train\"], \n",
    "                                    # batched=True,\n",
    "                                    # batch_size=1,\n",
    "                                    # num_proc=os.cpu_count()\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5208868-14e9-4011-af5d-1ea733782ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_dataset = senior_dataset.shuffle(seed=44)\n",
    "splited_senior_dataset = senior_dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e17169f-51ba-41c6-b0a6-e5e35e91068b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 1668\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 418\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_senior_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9985b-bd7d-4f08-990c-59a040f12f09",
   "metadata": {},
   "source": [
    "# 4. Set Trainer & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57264ea9-63ba-4f3d-9bfe-9a492fa9d9fc",
   "metadata": {},
   "source": [
    "## 4-1. Set DataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d6e60-8d7d-4ffe-9912-fc836926cd1d",
   "metadata": {},
   "source": [
    "    seq-to-seq modelì„ ìœ„í•œ DataCollatorëŠ” input_featureì™€ labelì„ ë…ë¦½ì ìœ¼ë¡œ ë‹¤ë£¬ë‹¤ëŠ” ì ì—ì„œ íŠ¹ì§•ì .\n",
    "        input_feature -> FeatureExtractor\n",
    "        label -> Tokenizer ë¡œ ë‹¤ë¤„ì•¼ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65a0d4c2-c3ba-4510-8421-9aedf7af51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_feature -> log_mel_spectrogram -> PyTorch Tensor\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# DatasetDict -> torch.tensor\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # ì¸í’‹ ë°ì´í„°ì™€ ë¼ë²¨ ë°ì´í„°ì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ë©°, ë”°ë¼ì„œ ì„œë¡œ ë‹¤ë¥¸ íŒ¨ë”© ë°©ë²•ì´ ì ìš©ë˜ì–´ì•¼ í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë‘ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ì•¼ í•œë‹¤.\n",
    "        # ë¨¼ì € ì˜¤ë””ì˜¤ ì¸í’‹ ë°ì´í„°ë¥¼ ê°„ë‹¨íˆ í† ì¹˜ í…ì„œë¡œ ë°˜í™˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # Tokenizeëœ ë ˆì´ë¸” ì‹œí€€ìŠ¤ë¥¼ ê°€ì ¸ì˜¨ë‹¤.\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # ë ˆì´ë¸” ì‹œí€€ìŠ¤ì— ëŒ€í•´ ìµœëŒ€ ê¸¸ì´ë§Œí¼ íŒ¨ë”© ì‘ì—…ì„ ì‹¤ì‹œí•œë‹¤.\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ loss ê³„ì‚° ê³¼ì •ì—ì„œ ë¬´ì‹œë˜ë„ë¡ í•œë‹¤.\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        # ì´ì „ í† í¬ë‚˜ì´ì¦ˆ ê³¼ì •ì—ì„œ bos í† í°ì´ ì¶”ê°€ë˜ì—ˆë‹¤ë©´ bos í† í°ì„ ì˜ë¼ë‚¸ë‹¤.\n",
    "        # í•´ë‹¹ í† í°ì€ ì´í›„ ì–¸ì œë“  ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41c592c9-e72e-4aad-9697-8181b4cbba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint, language=\"Korean\", task=\"transcribe\")\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be7cce-07f7-4832-aea8-6589b2265897",
   "metadata": {},
   "source": [
    "## 4-2. Set metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f22e791d-29a1-4d75-90c7-1b61c027a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('cer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29721f54-2358-4b48-b400-635117262c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    label_ids[label_ids == -11] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa980e-0cd0-4e6f-a413-0745fad7dab4",
   "metadata": {},
   "source": [
    "## 4-3. Set Train args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2b4a2-9963-4bbe-a79e-a369907784b8",
   "metadata": {},
   "source": [
    "    output_dir: model weightë¥¼ ì €ì¥í•˜ëŠ” ê²½ë¡œ\n",
    "    generation_max_length: evaluationë™ì•ˆ recursiveí•˜ê²Œ ìƒì„±ë˜ëŠ” tokenë“¤ì˜ ìµœëŒ€ ê¸¸ì´ \n",
    "    save_steps: Trainingë™ì•ˆ, ì´ parameterì— ì„¤ì •í•œ step ë§ˆë‹¤ checkpoint ì €ì¥\n",
    "    eval_steps: Trainingë™ì•ˆ, ì´ parameterì— ì„¤ì •í•œ step ë§ˆë‹¤ checkpoint í‰ê°€ ìˆ˜í–‰\n",
    "    report_to: training_logë¥¼ ì–´ë”” ì €ì¥í•  ì§€ ì„¤ì •. default: tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3af3d-e381-4693-aa6e-9946f7fed1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model load\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35a868cf-3929-4fa9-8199-03188f02ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdcc62de-ea8b-44ad-8564-935abac775a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wooseok/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "steps = 500\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./test_trainer\",       # output dir\n",
    "    # logging_dir=\"./logging_dir\",       # storing to log_dir\n",
    "    max_steps=steps,                   # epoch ëŒ€ì‹  ì„¤ì •\n",
    "    # num_train_epochs = 5,              # total num of training epochs\n",
    "    evaluation_strategy=\"steps\",       # evaluate after each \"steps\"\n",
    "\n",
    "    warmup_steps=steps/20,             # num of warmup steps for learning rate scheduler\n",
    "    save_steps=steps/10,\n",
    "    eval_steps=steps/20,\n",
    "    logging_steps=steps/50,            # how often to print log\n",
    "    \n",
    "    per_device_train_batch_size=16,    # batch size of per_device during \n",
    "    per_device_eval_batch_size=16,      # batch size per GPU for evaluation\n",
    "    # per_device_eval_batch_size=10,   # batch size for evaluation\n",
    "    gradient_accumulation_steps=2,     # tot num of steps before back propagation. train time increase.\n",
    "    learning_rate=1e-5,                # initial lr for AdamW optimizer\n",
    "    # weight_decay=0.01,                 # strength of weight decay\n",
    "    gradient_checkpointing=True,       # saving mem, but make backward pass slower\n",
    "    fp16=True,                         # use mixed precision\n",
    "    # fp16_opt_level=\"02\",               # mixed precision mode\n",
    "    \n",
    "    # predict_with_generate=True,        # using generate to calculate 'generative metrics'(ROUGE, BLEU)\n",
    "    # generation_max_length=225,         # generative metrics's max len\n",
    "\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,       # best checkpoint will always be saved\n",
    "    \n",
    "    metric_for_best_model=\"cer\",       # í•œêµ­ì–´ì˜ ê²½ìš° ë„ì–´ì“°ê¸°ê°€ ì• ë§¤í•œ ê²½ìš°ê°€ ë§ì•„ì„œ 'wer'ë³´ë‹¤ëŠ” 'cer'ì´ ë” ì í•©í•  ê²ƒ\n",
    "    greater_is_better=False,           # is better metric result is big?\n",
    "    push_to_hub=False,\n",
    "    # do_train=True,                   # Perform training\n",
    "    # do_eval=True,                    # Perform evaluation\n",
    "    run_name=\"test1800Training\",       # experiment name\n",
    "    # seed=3,                            # seed for experiment reproductbility 3x3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282679dc-f654-4767-bd2d-bf025b457117",
   "metadata": {},
   "source": [
    "## 4-4. Set Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccd798f9-c061-4c3e-8042-0c1bfd98126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=splited_senior_dataset[\"train\"],\n",
    "    eval_dataset=splited_senior_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fca102-0471-47a7-a0ae-a9d234213870",
   "metadata": {},
   "source": [
    "## 4-5. Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ec090ca-4861-4c87-b63c-66a4f0a3d344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ë§¨ ì²˜ìŒ set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b787f2b1-e385-43c8-9c27-22dd5cc3b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_DEVICE_ORDER: PCI_BUS_ID\n",
      "CUDA_VISIBLE_DEVICES: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA_DEVICE_ORDER:\", os.environ.get(\"CUDA_DEVICE_ORDER\"))\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57091c-edcf-488c-aef8-a4452d526bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch ë¥¼ ë¨¼ì € import í•˜ë©´ CUDA_VISIBLE_DEVICES ì„¤ì •ì´ ì•ˆë¨¹ìŒ\n",
    "import torch\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.config.use_cache = False # mem usage ì¤„ì´ê¸° ìœ„í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c477a3d9-6844-4c4e-8f5d-b661427a8bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aad58b-b915-428f-8106-928e7e4dd674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train() # data 1800 ê°œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4339e0b5-07b5-48f9-9f89-9524decb167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = WhisperForConditionalGeneration.from_pretrained(\"./test_trainer/checkpoint-150/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9829c2a-3878-45cf-8246-54ea6a447cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model_tuned,\n",
    "    train_dataset=splited_senior_dataset[\"train\"],\n",
    "    eval_dataset=splited_senior_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f497515-7f71-4239-8775-c51a294f81e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 01:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15303942561149597,\n",
       " 'eval_cer': 7.777777777777778,\n",
       " 'eval_runtime': 106.0101,\n",
       " 'eval_samples_per_second': 3.943,\n",
       " 'eval_steps_per_second': 0.5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4210fdab-69d6-4ca9-9cb8-210b624b2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw = WhisperForConditionalGeneration.from_pretrained(\"jiwon65/whisper-small_korean-zeroth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "794d049f-3975-4ab2-9397-d0625823a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = model_raw.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5450de5b-3143-4f53-9828-3c54b72e0f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1474889516830444, 'eval_cer': 29.860017497812773}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_kor",
   "language": "python",
   "name": "whisper_kor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
