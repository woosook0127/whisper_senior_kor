{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b32d48-dcd3-48ec-ae17-5d87ff86a29b",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ea3a6-7f90-42ad-bcdb-82c38e71e289",
   "metadata": {},
   "source": [
    "### Dataset: 민원(콜센터) 질의-응답 데이터\n",
    "### Using: Training and Validation of\n",
    "    ├─라벨링데이터_220121_add\n",
    "            │  │  ├─금융보험\n",
    "            │  │  │  ├─민원(콜센터) 질의응답_금융보험_사고 및 보상 문의_Training.zip | 4 MB | 174452\n",
    "            │  │  │  ├─민원(콜센터) 질의응답_금융보험_상품 가입 및 해지_Training.zip | 4 MB | 174453\n",
    "            │  │  │  ├─민원(콜센터) 질의응답_금융보험_이체출금대출서비스_Training.zip | 4 MB | 174454\n",
    "            │  │  │  └─민원(콜센터) 질의응답_금융보험_잔고 및 거래내역_Training.zip | 3 MB | 174455\n",
    "            │  │  ├─다산콜센터\n",
    "            │  │  │  ├─민원(콜센터) 질의응답_다산콜센터_대중교통 안내_Training.zip | 2 MB | 174456\n",
    "            │  │  │  ├─민원(콜센터) 질의응답_다산콜센터_생활하수도 관련 문의_Training.zip | 2 MB | 174457\n",
    "            │  │  │  ├─민원(콜센터) 질의응답_다산콜센터_일반행정 문의_Training.zip | 2 MB | 174458\n",
    "            │  │  │  └─민원(콜센터) 질의응답_다산콜센터_코로나19 관련 상담_Training.zip | 2 MB | 174459\n",
    "            │  │  └─질병관리본부\n",
    "            │  │      ├─민원(콜센터) 질의응답_질병관리본부_기타문의_Training.zip | 1 MB | 174461\n",
    "            │  │      ├─민원(콜센터) 질의응답_질병관리본부_약품식품_Training.zip | 497 KB | 174462\n",
    "            │  │      ├─민원(콜센터) 질의응답_질병관리본부_요양기관 현황_Training.zip | 789 KB | 174463\n",
    "            │  │      ├─민원(콜센터) 질의응답_질병관리본부_증상징후_Training.zip | 1 MB | 174464\n",
    "            │  │      └─민원(콜센터) 질의응답_질병관리본부_진료비정보_Training.zip | 294 KB | 174465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5c8537-c963-45f7-9435-ee4153dd00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files= {\n",
    "    \"train\": \"/data/whisper_testdata/QnA_data/Data/1.Training/라벨링데이터_220121_add/금융보험/*.zip\",\n",
    "\n",
    "    \"test\": \"/data/whisper_testdata/QnA_data/Data/2.Validation/라벨링데이터_220121_add/금융보험/*.zip\"\n",
    "}\n",
    "\n",
    "# json file 구조가 \"data\": [{data1}, {data2}, ...] 처럼 되어있지 않고 \n",
    "# {data1}, {data2}, ... 이므로 field=\"data\" 와 같이 필드 지정하면 오류남\n",
    "qna_dataset = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb93ab0-9029-441f-bdd9-acddd430a66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'고객질문(요청)': ['각 범위별로 설명 좀 해주시겠어요?',\n",
       "  '카드를 해지했는데, 그 해지한 카드 결제계좌에서 카드값이 나갔어요. 뭔 일이죠?',\n",
       "  '사지절단시는요?',\n",
       "  '',\n",
       "  '보험계약대출을 받으려고 하는데 어떻게 해야하나요? '],\n",
       " '고객답변': ['', '', '', '', ''],\n",
       " '화자': ['고객', '고객', '고객', '상담사', '고객'],\n",
       " '상담사의도': ['', '', '', '용종 제거', ''],\n",
       " '상담사답변': ['', '', '', '고객님, 본인확인부터 하겠습니다', ''],\n",
       " '지식베이스': ['설명,내용', '해지,결제금액', '절단,팔다리', '확인,검증', '보험계약대출,대부'],\n",
       " '도메인': ['금융/보험', '금융/보험', '금융/보험', '금융/보험', '금융/보험'],\n",
       " '고객의도': ['보험 가입 문의', '카드해지후승인내역', '교보생명', '', 'ATM기 대출'],\n",
       " '상담사질문(요청)': ['', '', '', '', ''],\n",
       " '카테고리': ['상품 가입 및 해지',\n",
       "  '잔고 및 거래내역',\n",
       "  '사고 및 보상 문의',\n",
       "  '사고 및 보상 문의',\n",
       "  '이체, 출금, 대출서비스'],\n",
       " '문장번호': ['11', '1', '17', '4', '1'],\n",
       " 'QA': ['Q', 'Q', 'Q', 'A', 'Q'],\n",
       " '용어사전': ['범위/영역,설명/내용',\n",
       "  '카드/전자화폐, 해지/취소, 카드값/결제금액',\n",
       "  '사지/팔다리',\n",
       "  '본인/당사자,확인/검증',\n",
       "  '대출/대부'],\n",
       " '대화셋일련번호': ['A8343', 'A18100', 'A19078', 'A8451', 'A6650'],\n",
       " '개체명 ': ['범위,설명', '카드, 해지, 결제계좌, 카드값', '사지,절단', '본인,확인', '보험계약 대출']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dataset_sample = qna_dataset[\"train\"].shuffle(seed=10).select(range(1000))\n",
    "qna_dataset_sample[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6771f07a-357f-42f7-8945-b15f37e72b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['고객질문(요청)', '고객답변', '화자', '상담사의도', '상담사답변', '지식베이스', '도메인', '고객의도', '상담사질문(요청)', '카테고리', '문장번호', 'QA', '용어사전', '대화셋일련번호', '개체명 '],\n",
       "        num_rows: 323215\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['고객질문(요청)', '고객답변', '화자', '상담사의도', '상담사답변', '지식베이스', '도메인', '고객의도', '상담사질문(요청)', '카테고리', '문장번호', 'QA', '용어사전', '대화셋일련번호', '개체명 '],\n",
       "        num_rows: 40426\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e72dfd8-a79f-4ebb-ba17-07be9d751ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31271, 31273, 31275, 31277, 31279, 31281, 36214, 38581, 53894, 75159, 75245, 78114, 85492, 85593, 85959, 96786, 122885, 124850, 134709, 134763, 140166, 141265, 149974, 150049, 150605, 158095, 159776, 161812, 162715, 172091, 172092, 173105, 173106, 176234, 176254, 197896, 198473, 199050, 199080, 199127, 207930, 207961, 208685, 208722, 214836, 215479, 215499, 215501, 226742, 226772, 226819, 234179, 234256, 237716, 248037, 251852, 282477, 301541, 317402]is empty\n"
     ]
    }
   ],
   "source": [
    "# 불필요한 data 버리기\n",
    "empty_indices = []\n",
    "\n",
    "for i, data in enumerate( qna_dataset[\"train\"]):      \n",
    "    if data[\"고객질문(요청)\"]+ data[\"고객답변\"] + data[\"상담사답변\"] +data[\"상담사질문(요청)\"] == '':\n",
    "        empty_indices.append(i)\n",
    "\n",
    "print(f\"{empty_indices}is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71f9f1e0-162d-4724-ac30-ad4665302d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n"
     ]
    }
   ],
   "source": [
    "for i in empty_indices:\n",
    "    print(qna_dataset[\"train\"][i][\"고객질문(요청)\"], qna_dataset[\"train\"][i][\"고객답변\"],qna_dataset[\"train\"][i][\"상담사답변\"],qna_dataset[\"train\"][i][\"상담사질문(요청)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f74b044-6a5f-4835-ab30-54dfe0d9a0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1edd115f6e48e3aa25f0adca19e230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/323215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4bd520be8b4aeda65703cf5f54e1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/40426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qna_dataset = qna_dataset.filter(lambda x, i: i not in empty_indices, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9cfc2c50-fdad-4a94-ae5c-b2393ac23479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57add5a5c8b94e20b19cac06d1121e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/323156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum_qna\u001b[39m(ex):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtalk\u001b[39m\u001b[38;5;124m\"\u001b[39m: ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m고객질문(요청)\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m고객답변\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m상담사답변\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m상담사질문(요청)\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m----> 5\u001b[0m qna_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mqna_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msum_qna\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 869\u001b[0m     {\n\u001b[1;32m    870\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    871\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    872\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    873\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    874\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    875\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    876\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    877\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    878\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    879\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    880\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    881\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    882\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    883\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    884\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    885\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    886\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    887\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    888\u001b[0m         )\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    890\u001b[0m     }\n\u001b[1;32m    891\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/dataset_dict.py:870\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    869\u001b[0m     {\n\u001b[0;32m--> 870\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    890\u001b[0m     }\n\u001b[1;32m    891\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/arrow_dataset.py:3533\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3531\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_row(example\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[1;32m   3532\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3533\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3534\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/arrow_writer.py:500\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/arrow_writer.py:447\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples):\n\u001b[1;32m    446\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m0\u001b[39m][col] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples]\n\u001b[0;32m--> 447\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    448\u001b[0m         chunk\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m (array\u001b[38;5;241m.\u001b[39mchunks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray) \u001b[38;5;28;01melse\u001b[39;00m [array])\n\u001b[1;32m    451\u001b[0m     ]\n\u001b[1;32m    452\u001b[0m     batch_examples[col] \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mconcat_arrays(arrays)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/datasets/arrow_writer.py:450\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples):\n\u001b[1;32m    446\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m0\u001b[39m][col] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples]\n\u001b[1;32m    447\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    448\u001b[0m         chunk\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m--> 450\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m (\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunks\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray) \u001b[38;5;28;01melse\u001b[39;00m [array])\n\u001b[1;32m    451\u001b[0m     ]\n\u001b[1;32m    452\u001b[0m     batch_examples[col] \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mconcat_arrays(arrays)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# QnA 합치기\n",
    "def sum_qna(ex):\n",
    "    return {\"talk\": ex[\"고객질문(요청)\"] + ex[\"고객답변\"] + ex[\"상담사답변\"] + ex[\"상담사질문(요청)\"]}\n",
    "\n",
    "qna_dataset = qna_dataset.map(sum_qna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a619631-13f2-45d7-9ec3-aea94e3c56ca",
   "metadata": {},
   "source": [
    "# 2. Set Tokenizer for data manipulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86338279-13ef-42f5-9103-eb8b6e5c7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jiwon65/whisper-small_korean-zeroth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "728c73a0-02cd-4808-8e2a-753499d22b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(ex):\n",
    "    return tokenizer(ex[\"talk\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c2296da-c741-4192-83c6-41cb88111eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fe6b28780346aab966a1751daf12c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/323156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511064be6d9841c99f0967d0bf9e2ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = qna_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d33016-4d97-42e4-a2f7-3201a600697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader 씌우기\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# batch 별 padding 을 위한 datacollator \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['고객질문(요청)'], ['고객답변'], ['화자'], ['상담사의도'], ['상담사답변'], ['지식베이스'], '도메인', '고객의도', '상담사질문(요청)', '카테고리', '문장번호', 'QA', '용어사전', '대화셋일련번호', '개체명 '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6320a-02d5-4d89-a84c-daf88f4ccd0b",
   "metadata": {},
   "source": [
    "# 3. Load model & Training\n",
    "### 3-1. Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9e60c6e-94f9-43c8-af4d-9a9feaf5e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wooseok/anaconda3/envs/whisper_kor/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "checkpoint = \"jiwon65/whisper-small_korean-zeroth\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68422323-a44f-4b5e-83c3-f8613b321b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# training hyperparameter\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"checkpoint_dir\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44cfc9ca-451f-410c-b478-6087998ed022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 mertic 설정\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096649bc-89db-4cba-9aad-5031df0e98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 로 훈련\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_kor",
   "language": "python",
   "name": "whisper_kor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
